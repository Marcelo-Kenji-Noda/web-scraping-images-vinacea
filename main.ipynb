{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import pyimpute\n",
    "import dask.dataframe as dd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection \n",
    "from sklearn import metrics\n",
    "from classes.domain import Domain\n",
    "from pyimpute import load_training_vector, load_targets, impute, evaluate_clf\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "\n",
    "map_display_center = {\"lat\": -16.95, \"lon\": -47.78}\n",
    "mapbox_style = \"carto-positron\"\n",
    "default_zoom = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Tratando os dados de ocorrências\n",
    "\n",
    "### Etapas\n",
    "- Filtrando somente as observações no Brasil\n",
    "- Removendo dados que não contém informações de data\n",
    "- Padronizando o nome dos estados\n",
    "- Padronizando a contagem como 1 para dados que contém apenas as ocorrencias\n",
    "- Criando um arquivo com as ocorrências de latitude e longitude de ocorrências\n",
    "\n",
    "### Saída:\n",
    "\n",
    "Arquivo gerado: ``assets/INPUT/occurence.parquet``\n",
    "\n",
    "Tabela gerada:\n",
    "| Latitude | Longitude | Presence |\n",
    "|----------|-----------|----------|\n",
    "|   -30    |    -30    |    1     |\n",
    "|   -28    |    -24    |    1     |\n",
    "|   -20    |    -31    |    1     |\n",
    "|   ...    |    ...    |   ...    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 1 - TRATAMENTO DE DADOS DE OCORRÊNCIAS\n",
    "Determinando o nome dos arquivos gerados que serão utilizados para as etapas seguintes\n",
    "\n",
    "Essa célula contém os dados constantes que podem ser alterados (Caminhos de arquivos e regras de renomeação de linhas)\n",
    "\"\"\"\n",
    "GBIF_RAW_FILE = \"assets/INPUT/gbif.csv\"\n",
    "GBIF_TREATED_FILE_OUTPUT = \"assets/INPUT/gbif.parquet\"\n",
    "OCCURANCE_FILE = \"assets/INPUT/occurence.parquet\"\n",
    "OCCURANCE_ABSENCE_FILE = \"assets/INPUT/occurence_abscence.parquet\"\n",
    "N_PSEUDO_ABSCENCE = 9_000\n",
    "\n",
    "STATE_NAME_ENCODING = {\n",
    "   \"Santa Catarina\":\"SC\",\n",
    "   \"São Paulo\":\"SP\",\n",
    "   \"Rio Grande do Sul\":\"RS\",\n",
    "   \"Minas Gerais\":\"MG\",\n",
    "   \"Paraná\":\"PR\",\n",
    "   \"Espírito Santo\":\"ES\",\n",
    "   \"Brazil - São Paulo\":\"SP\",\n",
    "   \"Rio de Janeiro\":\"RJ\",\n",
    "   \"Brazil - Minas Gerais\":\"MG\",\n",
    "   \"Bahia\":\"BA\",\n",
    "   \"Mato Grosso do Sul\":\"MS\",\n",
    "   \"Parana\":\"PR\",\n",
    "   \"Brazil - Santa Catarina\":\"SC\",\n",
    "   \"Sp\":\"SP\"  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting and transforming data to correct data types\n",
    "dtype_dict = {\n",
    "        'dateIdentified': 'object',\n",
    "       'day': 'float64',\n",
    "       'establishmentMeans': 'object',\n",
    "       'identifiedBy': 'object',\n",
    "       'mediaType': 'object',\n",
    "       'month': 'float64',\n",
    "       'recordNumber': 'object',\n",
    "       'rightsHolder': 'object',\n",
    "       'verbatimScientificNameAuthorship': 'object',\n",
    "       'year': 'float64'\n",
    "}\n",
    "\n",
    "## Lendo o arquivo original (gbif.csv) e salvando em um arquivo para ser tratado\n",
    "occurence_species_data_unfiltered = dd.read_csv(\n",
    "       GBIF_RAW_FILE,\n",
    "       sep='\\t',\n",
    "       dtype=dtype_dict\n",
    ").reset_index().compute().to_parquet(GBIF_TREATED_FILE_OUTPUT)\n",
    "\n",
    "## Selecionando somente as colunas que serão utilizadas no modelo e nas etapas de exploração de dados\n",
    "occurence_species_data = pd.read_parquet(\n",
    "    GBIF_TREATED_FILE_OUTPUT, \n",
    "    columns=[\n",
    "        'countryCode',\n",
    "        'locality',\n",
    "        'decimalLatitude',\n",
    "        'decimalLongitude',\n",
    "        'eventDate',\n",
    "        'individualCount',\n",
    "        'basisOfRecord',\n",
    "        'collectionCode',\n",
    "        'stateProvince'],\n",
    ").reset_index(drop=True)\n",
    "\n",
    "## Aplicando filtros e regras iniciais\n",
    "occurence_species_data = occurence_species_data[occurence_species_data['countryCode'] == 'BR'] # Filtering only in Brazil\n",
    "occurence_species_data = occurence_species_data[~occurence_species_data['eventDate'].isna()].reset_index(drop=True) # Removing data that does not contain date information\n",
    "occurence_species_data.loc[occurence_species_data['individualCount'].isna(),'individualCount'] = 1 # Caso um dado esteja no arquivo, porém não tenha um contagem de unidades específicas, o dado será considerado como 1 observação\n",
    "occurence_species_data = occurence_species_data[occurence_species_data['decimalLatitude'].notna() & occurence_species_data['decimalLongitude'].notna()] # Removing data without any information about latitude and longitude\n",
    "occurence_species_data['eventDate'] = pd.to_datetime(occurence_species_data['eventDate'], format=\"mixed\", utc=True) # Setting column as datetime\n",
    "occurence_species_data.loc[:,'stateProvince'] = occurence_species_data.stateProvince.replace(STATE_NAME_ENCODING) # Padronizando o nome dos estados\n",
    "occurence_species_data = occurence_species_data[~occurence_species_data['stateProvince'].isnull()].reset_index(drop=True) # Removendo linhas que não contém a informação de estado\n",
    "\n",
    "COLUMNS_RENAME = {\n",
    "    \"countryCode\":\"Pais\",\n",
    "    \"locality\":\"Localizacao\",\n",
    "    \"decimalLatitude\":\"Latitude\",\n",
    "    \"decimalLongitude\":\"Longitude\",\n",
    "    \"eventDate\":\"Data\",\n",
    "    \"individualCount\":\"Contagem de individuos\",\n",
    "    \"collectionCode\":\"Plataforma\",\n",
    "    \"stateProvince\":\"Estado\",\n",
    "    \"basisOfRecord\":\"Fonte do registro\"\n",
    "}\n",
    "\n",
    "# Renomeando os arquivos e criando a variável alvo \"Presence\"\n",
    "occurence_species_data.rename(columns=COLUMNS_RENAME, inplace=True)\n",
    "occurence_species_data.to_parquet(GBIF_TREATED_FILE_OUTPUT, index=False)\n",
    "occurence_species_data['Presence'] = 1\n",
    "occurence = occurence_species_data[['Latitude','Longitude','Presence']].copy()\n",
    "\n",
    "## São criados dois arquivos nesse momento, o occurence_file e o gbif_treated\n",
    "## O primeiro arquivo contém somente as informações de latitude, longitude e presença, que será utilizado no modelo\n",
    "## O segundo arquivo contém todas as informações tratadas para que seja possível realizar uma análise exploratória dos dados\n",
    "occurence.to_parquet(OCCURANCE_FILE, index=False)\n",
    "occurence_species_data.to_csv(\"generated_files/gbif_treated.csv\", index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 EDA\n",
    "\n",
    "(TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(occurence_species_data.head(2))\n",
    "\n",
    "del occurence_species_data_unfiltered, occurence_species_data, occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Criando pontos de pseudo-absência\n",
    "\n",
    "O método empregado será o de pontos de pseudo-absência, também conhecidos como pontos de background. Esses pontos não serão empregados para indicar diretamente a ausência da espécie em questão. Em vez disso, serão utilizados como uma referência para delimitar a extensão da área estudada.\n",
    "\n",
    "### Definindo o domínio dos pontos\n",
    "\n",
    "#### Opção 1\n",
    "Vamos determinar o domínio como sendo a caixa de recorte que envolve os estados do: [RS, SC, PR, SP, MG, ES, RJ]\n",
    "Definida por \n",
    "$x_{min}, y_{min}, x_{max},y_{max} = (-60, -33.7, -35, -10)$\n",
    "\n",
    "#### Opção 2\n",
    "A partir das observações, podemos determinar o menor polígono convexo que envolve todos esses pontos. Dessa forma, temos um domínio mais restrito para ser análisado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.domain import Domain\n",
    "\n",
    "domain = Domain(\n",
    "    bounds=(-60,-35,-35,-10),\n",
    "    output_path=\"assets/OUTPUT/\"\n",
    ")\n",
    "\n",
    "occurence_dataframe = pd.read_parquet(OCCURANCE_FILE)\n",
    "\n",
    "def generate_random_points(bounds: tuple, number: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Generating random points inside boundaries\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    minx, miny, maxx, maxy = bounds\n",
    "    x = np.random.uniform( minx, maxx, number )\n",
    "    y = np.random.uniform( miny, maxy, number )\n",
    "    return x, y\n",
    "\n",
    "x, y = generate_random_points(domain.bounds, N_PSEUDO_ABSCENCE)\n",
    "gdf_points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opcao_1 = True\n",
    "occurence_dataframe = pd.read_parquet(OCCURANCE_FILE)\n",
    "\n",
    "if opcao_1:\n",
    "    geoframe = gpd.read_file('assets\\FEATURES\\MALHAS\\BR_UF_2022.shp')\n",
    "    geoframe = geoframe[geoframe['SIGLA_UF'].isin(['MG','SP','RJ','ES','RS','SC'])]\n",
    "    domain_filter = geoframe.cx[:-42,:]\n",
    "else:\n",
    "    geoframe = pd.read_parquet(OCCURANCE_FILE)\n",
    "    geoframe = gpd.GeoDataFrame(geometry=gpd.points_from_xy(geoframe['Longitude'], geoframe['Latitude']))\n",
    "    domain_filter = geoframe.geometry.unary_union.convex_hull\n",
    "\n",
    "x, y = generate_random_points(geoframe.total_bounds, N_PSEUDO_ABSCENCE)\n",
    "gdf_points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x, y))\n",
    "absence_dataframe = gdf_points[gdf_points.within(domain_filter)]\n",
    "\n",
    "absence_dataframe['Latitude'] = absence_dataframe['geometry'].y\n",
    "absence_dataframe['Longitude'] = absence_dataframe['geometry'].x\n",
    "absence_dataframe['Presence'] = 0\n",
    "\n",
    "absence_dataframe = absence_dataframe[['Latitude','Longitude','Presence']]\n",
    "\n",
    "dataframe = pd.concat([absence_dataframe, occurence_dataframe])\n",
    "dataframe.to_parquet(OCCURANCE_ABSENCE_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(\n",
    "    dataframe,\n",
    "    lat=dataframe['Latitude'],\n",
    "    lon=dataframe['Longitude'],\n",
    "    center=map_display_center,\n",
    "    mapbox_style=mapbox_style,\n",
    "    color='Presence',\n",
    "    zoom=default_zoom,\n",
    "    opacity=1\n",
    ")\n",
    "fig.update_layout(\n",
    "    title_text= \"Mapa com pontos de ocorrências\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total de pontos gerados: \")\n",
    "print(dataframe['Presence'].value_counts())\n",
    "\n",
    "del dataframe, absence_dataframe, occurence_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Adicionando as features para os pontos gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe_with_raster_features(geoframe, explanatory_rasters, columns, output_file_path, response_field = \"Presence\"):\n",
    "    train_xs, train_y = load_training_vector(geoframe, explanatory_rasters, response_field=response_field)\n",
    "    df = pd.DataFrame(train_xs)\n",
    "    df.loc[:,response_field] = train_y\n",
    "    df = df[~df[0].isnull()]\n",
    "    df.columns = columns\n",
    "    df.to_parquet(output_file_path, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_features_rasters = [\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_1.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_2.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_3.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_4.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_5.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_6.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_7.tif'\n",
    "]\n",
    "precipitation_rasters = [\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_8.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_9.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_10.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_11.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_12.tif',\n",
    "]\n",
    "\n",
    "indice_aridez_umidade_rasters = [\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_13.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_14.tif',\n",
    "]\n",
    "numero_medio_de_dias_chuvosos = [\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_15.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_16.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_17.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_18.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_19.tif',\n",
    "]\n",
    "vegetacao = [\n",
    "    'assets\\FEATURES\\VEGETACAO\\Cultivated_and_Managed_Vegetation.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\Deciduous_Broadleaf_Trees.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\Evergreen_Broadleaf_Trees.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\Herbaceous_Vegetation.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\Mixed_Other_Trees.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\Open_Water.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\Regularly_Flooded_Vegetation.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\Shrubs.tif'\n",
    "]\n",
    "topografia = [\n",
    "    'assets\\FEATURES\\TOPOGRAFIA\\Elevation.tif',\n",
    "    'assets\\FEATURES\\TOPOGRAFIA\\Tangencial_curvature.tif'\n",
    "]\n",
    "\n",
    "rasters = [\n",
    "    temperature_features_rasters,\n",
    "    precipitation_rasters,\n",
    "    indice_aridez_umidade_rasters,\n",
    "    numero_medio_de_dias_chuvosos,\n",
    "    vegetacao\n",
    "]\n",
    "\n",
    "rasters_cols=[\n",
    "    ['BIO1','BIO2','BIO3','BIO4','BIO5','BIO6','BIO7','Presence'],\n",
    "    ['BIO8','BIO9','BIO10','BIO11','BIO12','Presence'],\n",
    "    ['BIO13','BIO14','Presence'],\n",
    "    ['BIO15','BIO16','BIO17','BIO18','BIO19','Presence'],\n",
    "    ['Cultivated_and_Managed_Vegetation','Deciduous_Broadleaf_Trees',\n",
    "     'Evergreen_Broadleaf_Trees','Herbaceous_Vegetation','Mixed_Other_Trees',\n",
    "     'Open_Water','Regularly_Flooded_Vegetation','Shrubs','Presence'\n",
    "    ],\n",
    "    ['Elevation','Tangencial_curvature','Presence']\n",
    "]\n",
    "\n",
    "raster_file_paths = [\n",
    "    'assets/INPUT/temp.parquet',\n",
    "    'assets/INPUT/preciptation.parquet',\n",
    "    'assets/INPUT/dias_chuvosos.parquet',\n",
    "    'assets/INPUT/vegetacao.parquet',\n",
    "    'assets/INPUT/topografia.parquet'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCCURENCE_GEO_JSON = \"assets/INPUT/occurences.json\"\n",
    "FEATURES_AND_TREATED_DATA = \"assets/INPUT/features.parquet\"\n",
    "coordinates = pd.read_parquet(OCCURANCE_ABSENCE_FILE)\n",
    "coordinates['geometry'] = list(zip(coordinates[\"Longitude\"], coordinates[\"Latitude\"]))\n",
    "coordinates = coordinates[['Presence','geometry']].copy()\n",
    "coordinates['geometry'] = coordinates[\"geometry\"].apply(Point)\n",
    "geo_dataframe = gpd.GeoDataFrame(coordinates)\n",
    "\n",
    "# Create the geodataframe\n",
    "OSD_geoframe = gpd.GeoDataFrame(\n",
    "    geo_dataframe,\n",
    "    crs = {'init': 'epsg:4326'},\n",
    "    geometry = geo_dataframe['geometry']\n",
    ")\n",
    "OSD_geoframe = OSD_geoframe.to_crs(\"EPSG:4326\")\n",
    "OSD_geoframe.reset_index(drop=True, inplace = True)\n",
    "OSD_geoframe.to_file(OCCURENCE_GEO_JSON, driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_FILE = True\n",
    "if DOWNLOAD_FILE:\n",
    "    for raster, raster_col, raster_output_file_path in zip(rasters, rasters_cols, raster_file_paths):\n",
    "        export_dataframe_with_raster_features(\n",
    "            geoframe=OCCURENCE_GEO_JSON,\n",
    "            explanatory_rasters=raster,\n",
    "            columns=raster_col,\n",
    "            output_file_path=raster_output_file_path)\n",
    "        print(f\"Arquivo salvo: {raster_output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Avaliando colinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criando um modelo\n",
    "\n",
    "### Maxent\n",
    "\n",
    "Etapas:\n",
    "1. Separar os dados em conjunto de treino e testes\n",
    "2. Ajustar o modelo\n",
    "3. Avaliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import model_selection \n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FEATURES_AND_TREATED_DATA = \"assets/INPUT/features.parquet\"\n",
    "\n",
    "# ROC \n",
    "def plot_roc_curve(fper, tper):\n",
    "    plt.plot(fper, tper, color='red', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Classifier evaluation functions (modify pyimpute function)\n",
    "def evaluate_clf(\n",
    "    clf, X, y, name, k=None, test_size=0.2, scoring=\"f1_weighted\", feature_names=None\n",
    "):\n",
    "    print(name)\n",
    "    X_train, X_test, y_train, y_true = model_selection.train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size, # Test data size\n",
    "        shuffle=True, # Shuffle the data before split\n",
    "        stratify=y # Keeping the appearance/non-appearance ratio of Y\n",
    "    )\n",
    "\n",
    "    if k: # Cross-validation\n",
    "        kf = model_selection.KFold(n_splits=k) # k-fold\n",
    "        scores = model_selection.cross_val_score(clf, X_train, y_train, cv=kf, scoring=scoring)\n",
    "        print(name + \" %d-fold Cross Validation Accuracy: %0.2f (+/- %0.2f)\"\n",
    "              % (k, scores.mean() * 100, scores.std() * 200))\n",
    "        print()\n",
    "    \n",
    "    clf.fit(X_train, y_train) # Training of classifiers\n",
    "    y_pred = clf.predict(X_test) # Classifier predictions\n",
    "    \n",
    "    # Classifier evaluation metrics\n",
    "    print(\"Accuracy Score: %.2f\" % metrics.accuracy_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    print(\"Classification report\")\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    print(\"Confussion matrix\")\n",
    "    print(metrics.confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "    \n",
    "    print('AUC(ROC): %.2f'% metrics.roc_auc_score(y_true, y_pred))\n",
    "    print()\n",
    "       \n",
    "    # ROC \n",
    "    probs = clf.predict_proba(X_test) \n",
    "    prob = probs[:, 1]  \n",
    "    fper, tper, thresholds = metrics.roc_curve(y_true, prob)\n",
    "    plot_roc_curve(fper, tper)\n",
    "\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        print(\"Feature importances\")\n",
    "        for f, imp in zip(feature_names, clf.feature_importances_):\n",
    "            print(\"%20s: %s\" % (f, round(imp * 100, 1)))\n",
    "        print()\n",
    "    return clf\n",
    "FEATURES_AND_TREATED_DATA = \"assets/INPUT/features.parquet\"\n",
    "df = pd.read_parquet(FEATURES_AND_TREATED_DATA)\n",
    "df = df.dropna()\n",
    "\n",
    "y = df.pop(\"Presence\")\n",
    "X = df.copy()\n",
    "X = df[['Anual_Mean_Temp','Mean_Diurnal_Range','Temperature_Anual_Range','Anual_Preciptation','Broadleaf_Trees','Geomflat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent = LogisticRegression(max_iter=1_000)\n",
    "maxent = evaluate_clf(maxent, X, y, \"MaxEnt\", k=5, test_size=0.2, scoring=\"f1_weighted\", feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "random_forest = evaluate_clf(random_forest, X, y, \"Random Forest\", k=5, test_size=0.2, scoring=\"f1_weighted\", feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "geoframe = gpd.read_file('assets\\FEATURES\\MALHAS\\BR_UF_2022.shp')\n",
    "geoframe = geoframe[geoframe['SIGLA_UF'].isin(['MG','SP','RJ','ES'])]\n",
    "geoframe = geoframe.cx[:-42,:]\n",
    "\n",
    "OCCURENCE_GEO_JSON_PREDICT = \"assets/INPUT/predict.json\"\n",
    "FEATURES_AND_TREATED_DATA_PREDICT = \"assets/INPUT/features_predict.parquet\"\n",
    "\n",
    "domain_saida = Domain(\n",
    "    bounds=(-46,-25,-39,-18),\n",
    "    output_path=\"assets/MASKED_FEATURES_SHP/\"\n",
    ")\n",
    "\n",
    "explanatory_rasters = [\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_1.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_2.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_7.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_12.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\evergreen_broadleaf_trees.tif',\n",
    "    'assets\\FEATURES\\TOPOGRAFIA\\geomflat_1KMperc_GMTEDmd.tif'\n",
    "]\n",
    "\n",
    "new_explanatory_rasters = [\n",
    "    'assets/MASKED_FEATURES_SHP/wc2.1_30s_bio_1.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/wc2.1_30s_bio_2.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/wc2.1_30s_bio_7.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/wc2.1_30s_bio_12.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/evergreen_broadleaf_trees.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/geomflat_1KMperc_GMTEDmd.tif'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in explanatory_rasters:\n",
    "    domain_saida.mask_tiff_with_shapefile(tiff_file=file,geoframe = geoframe, output_file_name=Path(file).name)\n",
    "    \n",
    "target_xs, raster_info = pyimpute.load_targets(new_explanatory_rasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyimpute.impute(target_xs, maxent, raster_info, outdir='OUTPUT/' + 'MAXENT' + '-IMAGES', class_prob=True, certainty=True)\n",
    "pyimpute.impute(target_xs, random_forest, raster_info, outdir='OUTPUT/' + 'RF' + '-IMAGES', class_prob=True, certainty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotit(x, title, cmap=\"Blues\"):\n",
    "    plt.figure(figsize = (14,7))\n",
    "    plt.imshow(x, cmap=cmap, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.title(title, fontweight = 'bold')\n",
    "    \n",
    "distr_rf = rasterio.open(\"OUTPUT\\MAXENT-IMAGES\\probability_1.tif\").read(1) \n",
    "distr_et = rasterio.open(\"OUTPUT\\RF-IMAGES\\probability_1.tif\").read(1)\n",
    "\n",
    "distr_averaged = (distr_rf + distr_et)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "def calculate_tiff_mean(tiff_file, minx, miny, maxx, maxy):\n",
    "    # Abrir o arquivo .tiff\n",
    "    with rasterio.open(tiff_file) as src:\n",
    "        # Recortar o raster com a geometria do polígono\n",
    "        bbox = [{\n",
    "            'type': 'Polygon',\n",
    "            'coordinates': [[\n",
    "                (minx, miny),\n",
    "                (minx, maxy),\n",
    "                (maxx, maxy),\n",
    "                (maxx, miny),\n",
    "                (minx, miny)\n",
    "            ]]\n",
    "        }]\n",
    "        try:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, bbox, crop=True, nodata=np.nan)\n",
    "            # Calcular a média dos valores dentro do polígono\n",
    "            mean_value = np.nanmean(out_image)\n",
    "        except:\n",
    "            return 0\n",
    "    return mean_value\n",
    "\n",
    "def create_grid_with_contours(gdf, n_cols, n_rows, tiff_file):\n",
    "    # Obter limites do shapefile\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    #xmin, ymin, xmax, ymax = [-53.11011153, -41, -28.84763991, -14.23318067]\n",
    "    # Calcular o tamanho dos retângulos\n",
    "    dx = (xmax - xmin) / n_cols\n",
    "    dy = (ymax - ymin) / n_rows\n",
    "\n",
    "    # Lista para armazenar polígonos\n",
    "    polygons = []\n",
    "    mean_val = []\n",
    "    x_min = []\n",
    "    x_max = []\n",
    "    y_min = []\n",
    "    y_max = []\n",
    "    # Loop para criar os polígonos\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            x0 = xmin + j * dx\n",
    "            y0 = ymin + i * dy\n",
    "            x1 = x0 + dx\n",
    "            y1 = y0 + dy\n",
    "            \n",
    "            x_min.append(x0)\n",
    "            y_min.append(y0)\n",
    "            x_max.append(x1)\n",
    "            y_max.append(y1)\n",
    "            polygons.append(Polygon([(x0, y0), (x1, y0), (x1, y1), (x0, y1)]))\n",
    "            mean_val.append(calculate_tiff_mean(tiff_file,x0, y0, x1, y1))\n",
    "    # Criar GeoDataFrame a partir dos polígonos\n",
    "    grid_gdf = gpd.GeoDataFrame(geometry=polygons, crs=gdf.crs)\n",
    "    grid_gdf['x_min'] = x_min\n",
    "    grid_gdf['y_min'] = y_min\n",
    "    grid_gdf['x_max'] = x_max\n",
    "    grid_gdf['y_max'] = y_max\n",
    "    grid_gdf['mean_val'] = mean_val\n",
    "    # Interseção com o shapefile original\n",
    "    grid_gdf = gpd.overlay(grid_gdf, gdf, how='intersection')\n",
    "\n",
    "    return grid_gdf\n",
    "\n",
    "# Caminho para o arquivo .shp\n",
    "tiff_file = 'OUTPUT\\MAXENT-IMAGES\\probability_1.tif'\n",
    "\n",
    "# Número de colunas e linhas na malha\n",
    "n_cols = 50\n",
    "n_rows = 50\n",
    "\n",
    "# Criar a malha quadriculada com contornos\n",
    "grid_with_contours = create_grid_with_contours(geoframe.cx[:-42,:], n_cols, n_rows, tiff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar a malha quadriculada com contornos\n",
    "grid_with_contours.plot(column='mean_val', edgecolor='black', cmap='viridis', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotar o heatmap com Plotly Express\n",
    "fig = px.choropleth_mapbox(grid_with_contours, geojson=grid_with_contours.geometry.__geo_interface__,\n",
    "                           locations=grid_with_contours.index, color='mean_val',\n",
    "                           color_continuous_scale=\"viridis\",\n",
    "                           range_color=(0, 1),\n",
    "                           center={\"lat\": -16.95, \"lon\": -47.78},\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=3,\n",
    "                           opacity=1,\n",
    "                           labels={'mean_val': 'Mean Value'}\n",
    "                          )\n",
    "# Adicionar contorno dos polígonos\n",
    "OCCURANCE_FILE = \"assets/INPUT/occurence.parquet\"\n",
    "occurence_df = pd.read_parquet(OCCURANCE_FILE)\n",
    "with open(\"assets/INPUT/occurences.json\") as jfile:\n",
    "    geo_json = json.load(jfile)\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "        lat=occurence_df['Latitude'],\n",
    "        lon=occurence_df['Longitude'],\n",
    "        mode='markers',\n",
    "        marker=go.scattermapbox.Marker(\n",
    "            size=8,\n",
    "            color='rgb(0, 0, 0)',\n",
    "            opacity=1\n",
    "        ),\n",
    "        hoverinfo='none'\n",
    "    ))\n",
    "\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "        lat=occurence_df['Latitude'],\n",
    "        lon=occurence_df['Longitude'],\n",
    "        mode='markers',\n",
    "        marker=go.scattermapbox.Marker(\n",
    "            size=6,\n",
    "            color='rgb(194, 33, 36)',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        hoverinfo='none'\n",
    "))\n",
    "\n",
    "fig.update_geos(\n",
    "    visible=False,\n",
    "    resolution=50,\n",
    "    lataxis_gridcolor=\"red\",\n",
    "    lataxis_showgrid=True,\n",
    "    lataxis_dtick=15,\n",
    "    lonaxis_showgrid=True,\n",
    "    lonaxis_dtick=15,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotit(distr_averaged, \"Species Suitability\", cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_AND_TREATED_DATA = \"assets/INPUT/features.parquet\"\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "df = pd.read_parquet(FEATURES_AND_TREATED_DATA)\n",
    "df = df.dropna()\n",
    "\n",
    "y = df.pop(\"Presence\")\n",
    "X = df.copy()\n",
    "\n",
    "def apply_pca(X, standardize=True):\n",
    "    # Standardize\n",
    "    if standardize:\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Create principal components\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    # Create loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,  # transpose the matrix of loadings\n",
    "        columns=component_names,  # so the columns are the principal components\n",
    "        index=X.columns,  # and the rows are the original features\n",
    "    )\n",
    "    return pca, X_pca, loadings\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "pca, X_pca, loadings = apply_pca(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variance(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mi_scores=make_mi_scores(X, y)\n",
    "fig = go.Figure([go.Bar(\n",
    "    x=mi_scores.index, y=mi_scores.values\n",
    ")])\n",
    "fig.update_layout(\n",
    "    \n",
    ")\n",
    "fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validando multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de multicolinearidade\n",
    "\n",
    "Analisando se os parâmetros são os mesmos nas condições de fitar o modelo com ou sem uma das variáveis.\n",
    "\n",
    "Se eles são não correlacionados, então os coeficientes devem ser iguais para o caso em que ambos estão x quando X1 está e quando ambos estão x quando X2 está\n",
    "\n",
    "$$\n",
    "SSR(X_1|X_2) = SSR(X_1) = SSE(X_2) - SSE(X_1,X_2)\n",
    "$$\n",
    "SSR(X)\n",
    "\n",
    "##\n",
    "Quando existem duas variáveis independentes correlacionadas, os coeficientes da regressão dependem se um ou outra variável está inclusida no modelo\n",
    "\n",
    "- $SSR(X_1)$: Redução de variação da variável resposta ao introduzir a variável $X_1$.\n",
    "- $SSR(X_1|X_2)$: Corresponde a redução de variação da resposta ao introduzir a variável $X_1$ dado que a $X_2$ já está no modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web-scraping-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
