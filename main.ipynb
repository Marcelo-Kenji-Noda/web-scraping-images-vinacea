{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import pyimpute\n",
    "import dask.dataframe as dd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection \n",
    "from sklearn import metrics\n",
    "from classes.domain import Domain\n",
    "from pyimpute import load_training_vector, load_targets, impute, evaluate_clf\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Tratando os dados de ocorrências\n",
    "\n",
    "### Etapas\n",
    "- Filtrando somente as observações no Brasil\n",
    "- Removendo dados que não contém informações de data\n",
    "- Padronizando o nome dos estados\n",
    "- Padronizando a contagem como 1 para dados que contém apenas as ocorrencias\n",
    "- Criando um arquivo com as ocorrências de latitude e longitude de ocorrências\n",
    "\n",
    "### Saída:\n",
    "\n",
    "Arquivo gerado: ``assets/INPUT/occurence.parquet``\n",
    "\n",
    "Tabela gerada:\n",
    "| Latitude | Longitude | Presence |\n",
    "|----------|-----------|----------|\n",
    "|   -30    |    -30    |    1     |\n",
    "|   -28    |    -24    |    1     |\n",
    "|   -20    |    -31    |    1     |\n",
    "|   ...    |    ...    |   ...    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBIF_RAW_FILE = \"assets/INPUT/gbif.csv\"\n",
    "GBIF_TREATED_FILE_OUTPUT = \"assets/INPUT/gbif.parquet\"\n",
    "OCCURANCE_FILE = \"assets/INPUT/occurence.parquet\"\n",
    "OCCURANCE_ABSENCE_FILE = \"assets/INPUT/occurence_abscence.parquet\"\n",
    "N_PSEUDO_ABSCENCE = 9_000\n",
    "\n",
    "STATE_NAME_ENCODING = {\n",
    "   \"Santa Catarina\":\"SC\",\n",
    "   \"São Paulo\":\"SP\",\n",
    "   \"Rio Grande do Sul\":\"RS\",\n",
    "   \"Minas Gerais\":\"MG\",\n",
    "   \"Paraná\":\"PR\",\n",
    "   \"Espírito Santo\":\"ES\",\n",
    "   \"Brazil - São Paulo\":\"SP\",\n",
    "   \"Rio de Janeiro\":\"RJ\",\n",
    "   \"Brazil - Minas Gerais\":\"MG\",\n",
    "   \"Bahia\":\"BA\",\n",
    "   \"Mato Grosso do Sul\":\"MS\",\n",
    "   \"Parana\":\"PR\",\n",
    "   \"Brazil - Santa Catarina\":\"SC\",\n",
    "   \"Sp\":\"SP\"  \n",
    "}\n",
    "\n",
    "dtype_dict = {\n",
    "        'dateIdentified': 'object',\n",
    "       'day': 'float64',\n",
    "       'establishmentMeans': 'object',\n",
    "       'identifiedBy': 'object',\n",
    "       'mediaType': 'object',\n",
    "       'month': 'float64',\n",
    "       'recordNumber': 'object',\n",
    "       'rightsHolder': 'object',\n",
    "       'verbatimScientificNameAuthorship': 'object',\n",
    "       'year': 'float64'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting and transforming data to correct data types\n",
    "occurence_species_data_unfiltered = dd.read_csv(\n",
    "       GBIF_RAW_FILE,\n",
    "       sep='\\t',\n",
    "       dtype=dtype_dict\n",
    ").reset_index().compute().to_parquet(GBIF_TREATED_FILE_OUTPUT)\n",
    "\n",
    "occurence_species_data = pd.read_parquet(\n",
    "    GBIF_TREATED_FILE_OUTPUT, \n",
    "    columns=[\n",
    "        'countryCode',\n",
    "        'locality',\n",
    "        'decimalLatitude',\n",
    "        'decimalLongitude',\n",
    "        'eventDate',\n",
    "        'individualCount',\n",
    "        'basisOfRecord',\n",
    "        'collectionCode',\n",
    "        'stateProvince'],\n",
    ").reset_index(drop=True)\n",
    "\n",
    "occurence_species_data = occurence_species_data[occurence_species_data['countryCode'] == 'BR'] # Filtering only in Brazil\n",
    "occurence_species_data = occurence_species_data[~occurence_species_data['eventDate'].isna()].reset_index(drop=True) # Removing data that does not contain date information\n",
    "occurence_species_data.loc[occurence_species_data['individualCount'].isna(),'individualCount'] = 1 # Setting 1 as default\n",
    "occurence_species_data = occurence_species_data[occurence_species_data['decimalLatitude'].notna() & occurence_species_data['decimalLongitude'].notna()] # Removing data without any information about latitude and longitude\n",
    "occurence_species_data['eventDate'] = pd.to_datetime(occurence_species_data['eventDate'], format=\"mixed\", utc=True) # Setting column as datetime\n",
    "occurence_species_data.loc[:,'stateProvince'] = occurence_species_data.stateProvince.replace(STATE_NAME_ENCODING) # Renaming\n",
    "\n",
    "occurence_species_data = occurence_species_data[~occurence_species_data['stateProvince'].isnull()].reset_index(drop=True)\n",
    "\n",
    "COLUMNS_RENAME = {\n",
    "    \"countryCode\":\"Pais\",\n",
    "    \"locality\":\"Localizacao\",\n",
    "    \"decimalLatitude\":\"Latitude\",\n",
    "    \"decimalLongitude\":\"Longitude\",\n",
    "    \"eventDate\":\"Data\",\n",
    "    \"individualCount\":\"Contagem de individuos\",\n",
    "    \"collectionCode\":\"Plataforma\",\n",
    "    \"stateProvince\":\"Estado\",\n",
    "    \"basisOfRecord\":\"Fonte do registro\"\n",
    "}\n",
    "\n",
    "occurence_species_data.rename(columns=COLUMNS_RENAME, inplace=True)\n",
    "occurence_species_data.to_parquet(GBIF_TREATED_FILE_OUTPUT, index=False)\n",
    "occurence_species_data['Presence'] = 1\n",
    "occurence = occurence_species_data[['Latitude','Longitude','Presence']].copy()\n",
    "\n",
    "occurence.to_parquet(OCCURANCE_FILE, index=False)\n",
    "occurence_species_data.to_csv(\"generated_files/gbif_treated.csv\", index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(occurence_species_data.head(2))\n",
    "\n",
    "del occurence_species_data_unfiltered, occurence_species_data, occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Criando pontos de pseudo-absência\n",
    "\n",
    "### Definindo o domínio dos pontos\n",
    "Vamos determinar o domínio como sendo a caixa de recorte que envolve os estados do: [RS, SC, PR, SP, MG, ES, RJ]\n",
    "\n",
    "Definida por \n",
    "\n",
    "$x_{min}, y_{min}, x_{max},y_{max} = (-60, -33.7, -35, -10)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.domain import Domain\n",
    "\n",
    "domain = Domain(\n",
    "    bounds=(-60,-35,-35,-10),\n",
    "    output_path=\"assets/OUTPUT/\"\n",
    ")\n",
    "\n",
    "occurence_dataframe = pd.read_parquet(OCCURANCE_FILE)\n",
    "\n",
    "def generate_random_points(bounds: tuple, number: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Generating random points inside boundaries\n",
    "    \"\"\"   \n",
    "    minx, miny, maxx, maxy = bounds\n",
    "    x = np.random.uniform( minx, maxx, number )\n",
    "    y = np.random.uniform( miny, maxy, number )\n",
    "    return x, y\n",
    "\n",
    "x, y = generate_random_points(domain.bounds, N_PSEUDO_ABSCENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurence_dataframe = pd.read_parquet(OCCURANCE_FILE)\n",
    "geoframe = pd.read_parquet(OCCURANCE_FILE)\n",
    "geoframe = gpd.GeoDataFrame(geometry=gpd.points_from_xy(geoframe['Longitude'], geoframe['Latitude']))\n",
    "\n",
    "x, y = generate_random_points(geoframe.total_bounds, N_PSEUDO_ABSCENCE)\n",
    "gdf_points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x, y))\n",
    "\n",
    "\n",
    "#geoframe = gpd.read_file('assets\\FEATURES\\MALHAS\\BR_UF_2022.shp')\n",
    "#geoframe = geoframe[geoframe['SIGLA_UF'].isin(['MG','SP','RJ','ES','RS','SC])]\n",
    "#geoframe = geoframe.cx[:-42,:]\n",
    "# Carregue o shapefile\n",
    "convex_hull = geoframe.geometry.unary_union.convex_hull\n",
    "# Use a função contains para filtrar os pontos contidos no shapefile\n",
    "absence_dataframe = gdf_points[gdf_points.within(convex_hull)]\n",
    "\n",
    "absence_dataframe['Latitude'] = absence_dataframe['geometry'].y\n",
    "absence_dataframe['Longitude'] = absence_dataframe['geometry'].x\n",
    "absence_dataframe['Presence'] = 0\n",
    "\n",
    "absence_dataframe = absence_dataframe[['Latitude','Longitude','Presence']]\n",
    "\n",
    "dataframe = pd.concat([absence_dataframe, occurence_dataframe])\n",
    "dataframe.to_parquet(OCCURANCE_ABSENCE_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(\n",
    "    dataframe,\n",
    "    lat=dataframe['Latitude'],\n",
    "    lon=dataframe['Longitude'],\n",
    "    center={\"lat\": -16.95, \"lon\": -47.78},\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    color='Presence',\n",
    "    zoom=3,\n",
    "    opacity=1\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe['Presence'].value_counts())\n",
    "\n",
    "del dataframe, absence_dataframe, occurence_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Adicionando as features para os pontos gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCCURENCE_GEO_JSON = \"assets/INPUT/occurences.json\"\n",
    "FEATURES_AND_TREATED_DATA = \"assets/INPUT/features.parquet\"\n",
    "explanatory_rasters = [\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_1.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_2.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_7.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_12.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\evergreen_broadleaf_trees.tif',\n",
    "    'assets\\FEATURES\\ENHANCED_VEGETATION_INDEX\\evenness_01_05_1km_uint16.tif',\n",
    "    'assets\\FEATURES\\FLAT_TOPOGRAPGIC_CATEGORICAL\\geomflat_1KMperc_GMTEDmd.tif'\n",
    "]\n",
    "\n",
    "coordinates = pd.read_parquet(OCCURANCE_ABSENCE_FILE)\n",
    "\n",
    "coordinates['geometry'] = list(zip(coordinates[\"Longitude\"], coordinates[\"Latitude\"]))\n",
    "coordinates = coordinates[['Presence','geometry']].copy()\n",
    "coordinates['geometry'] = coordinates[\"geometry\"].apply(Point)\n",
    "geo_dataframe = gpd.GeoDataFrame(coordinates)\n",
    "\n",
    "# Create the geodataframe\n",
    "OSD_geoframe = gpd.GeoDataFrame(\n",
    "    geo_dataframe,\n",
    "    crs = {'init': 'epsg:4326'},\n",
    "    geometry = geo_dataframe['geometry']\n",
    ")\n",
    "OSD_geoframe = OSD_geoframe.to_crs(\"EPSG:4326\")\n",
    "OSD_geoframe.reset_index(drop=True, inplace = True)\n",
    "OSD_geoframe.to_file(OCCURENCE_GEO_JSON, driver=\"GeoJSON\")\n",
    "\n",
    "train_xs, train_y = load_training_vector(OCCURENCE_GEO_JSON, explanatory_rasters, response_field=\"Presence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(train_xs)\n",
    "df.loc[:,'Presence'] = train_y\n",
    "df = df[~df[0].isnull()]\n",
    "df.columns = ['Anual_Mean_Temp', 'Mean_Diurnal_Range', 'Temperature_Anual_Range', 'Anual_Preciptation','Broadleaf_Trees', 'Eveness', 'Geomflat', 'Presence']\n",
    "df.to_parquet(FEATURES_AND_TREATED_DATA, index=False)\n",
    "\n",
    "del coordinates, OSD_geoframe, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criando um modelo\n",
    "\n",
    "### Maxent\n",
    "\n",
    "Etapas:\n",
    "1. Separar os dados em conjunto de treino e testes\n",
    "2. Ajustar o modelo\n",
    "3. Avaliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import model_selection \n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FEATURES_AND_TREATED_DATA = \"assets/INPUT/features.parquet\"\n",
    "\n",
    "# ROC \n",
    "def plot_roc_curve(fper, tper):\n",
    "    plt.plot(fper, tper, color='red', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Classifier evaluation functions (modify pyimpute function)\n",
    "def evaluate_clf(\n",
    "    clf, X, y, name, k=None, test_size=0.2, scoring=\"f1_weighted\", feature_names=None\n",
    "):\n",
    "    print(name)\n",
    "    X_train, X_test, y_train, y_true = model_selection.train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size, # Test data size\n",
    "        shuffle=True, # Shuffle the data before split\n",
    "        stratify=y # Keeping the appearance/non-appearance ratio of Y\n",
    "    )\n",
    "\n",
    "    if k: # Cross-validation\n",
    "        kf = model_selection.KFold(n_splits=k) # k-fold\n",
    "        scores = model_selection.cross_val_score(clf, X_train, y_train, cv=kf, scoring=scoring)\n",
    "        print(name + \" %d-fold Cross Validation Accuracy: %0.2f (+/- %0.2f)\"\n",
    "              % (k, scores.mean() * 100, scores.std() * 200))\n",
    "        print()\n",
    "    \n",
    "    clf.fit(X_train, y_train) # Training of classifiers\n",
    "    y_pred = clf.predict(X_test) # Classifier predictions\n",
    "    \n",
    "    # Classifier evaluation metrics\n",
    "    print(\"Accuracy Score: %.2f\" % metrics.accuracy_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    print(\"Classification report\")\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    print(\"Confussion matrix\")\n",
    "    print(metrics.confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "    \n",
    "    print('AUC(ROC): %.2f'% metrics.roc_auc_score(y_true, y_pred))\n",
    "    print()\n",
    "       \n",
    "    # ROC \n",
    "    probs = clf.predict_proba(X_test) \n",
    "    prob = probs[:, 1]  \n",
    "    fper, tper, thresholds = metrics.roc_curve(y_true, prob)\n",
    "    plot_roc_curve(fper, tper)\n",
    "\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        print(\"Feature importances\")\n",
    "        for f, imp in zip(feature_names, clf.feature_importances_):\n",
    "            print(\"%20s: %s\" % (f, round(imp * 100, 1)))\n",
    "        print()\n",
    "    return clf\n",
    "FEATURES_AND_TREATED_DATA = \"assets/INPUT/features.parquet\"\n",
    "df = pd.read_parquet(FEATURES_AND_TREATED_DATA)\n",
    "df = df.dropna()\n",
    "\n",
    "y = df.pop(\"Presence\")\n",
    "X = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent = LogisticRegression(max_iter=1_000)\n",
    "maxent = evaluate_clf(maxent, X, y, \"MaxEnt\", k=5, test_size=0.2, scoring=\"f1_weighted\", feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "random_forest = evaluate_clf(random_forest, X, y, \"Random Forest\", k=5, test_size=0.2, scoring=\"f1_weighted\", feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "geoframe = gpd.read_file('assets\\FEATURES\\MALHAS\\BR_UF_2022.shp')\n",
    "geoframe = geoframe[geoframe['SIGLA_UF'].isin(['MG','SP','RJ','ES'])]\n",
    "geoframe = geoframe.cx[:-42,:]\n",
    "\n",
    "OCCURENCE_GEO_JSON_PREDICT = \"assets/INPUT/predict.json\"\n",
    "FEATURES_AND_TREATED_DATA_PREDICT = \"assets/INPUT/features_predict.parquet\"\n",
    "\n",
    "domain_saida = Domain(\n",
    "    bounds=(-46,-25,-39,-18),\n",
    "    output_path=\"assets/MASKED_FEATURES_SHP/\"\n",
    ")\n",
    "\n",
    "explanatory_rasters = [\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_1.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_2.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_7.tif',\n",
    "    'assets\\FEATURES\\TEMPERATURE\\wc2.1_30s_bio_12.tif',\n",
    "    'assets\\FEATURES\\VEGETACAO\\evergreen_broadleaf_trees.tif',\n",
    "    'assets\\FEATURES\\ENHANCED_VEGETATION_INDEX\\evenness_01_05_1km_uint16.tif',\n",
    "    'assets\\FEATURES\\FLAT_TOPOGRAPGIC_CATEGORICAL\\geomflat_1KMperc_GMTEDmd.tif'\n",
    "]\n",
    "\n",
    "new_explanatory_rasters = [\n",
    "    'assets/MASKED_FEATURES_SHP/wc2.1_30s_bio_1.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/wc2.1_30s_bio_2.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/wc2.1_30s_bio_7.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/wc2.1_30s_bio_12.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/evergreen_broadleaf_trees.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/evenness_01_05_1km_uint16.tif',\n",
    "    'assets/MASKED_FEATURES_SHP/geomflat_1KMperc_GMTEDmd.tif'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in explanatory_rasters:\n",
    "    domain_saida.mask_tiff_with_shapefile(tiff_file=file,geoframe = geoframe, output_file_name=Path(file).name)\n",
    "    \n",
    "target_xs, raster_info = pyimpute.load_targets(new_explanatory_rasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyimpute.impute(target_xs, maxent, raster_info, outdir='OUTPUT/' + 'MAXENT' + '-IMAGES', class_prob=True, certainty=True)\n",
    "pyimpute.impute(target_xs, random_forest, raster_info, outdir='OUTPUT/' + 'RF' + '-IMAGES', class_prob=True, certainty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotit(x, title, cmap=\"Blues\"):\n",
    "    plt.figure(figsize = (14,7))\n",
    "    plt.imshow(x, cmap=cmap, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.title(title, fontweight = 'bold')\n",
    "    \n",
    "distr_rf = rasterio.open(\"OUTPUT\\MAXENT-IMAGES\\probability_1.tif\").read(1) \n",
    "distr_et = rasterio.open(\"OUTPUT\\RF-IMAGES\\probability_1.tif\").read(1)\n",
    "\n",
    "distr_averaged = (distr_rf + distr_et)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "def calculate_tiff_mean(tiff_file, minx, miny, maxx, maxy):\n",
    "    # Abrir o arquivo .tiff\n",
    "    with rasterio.open(tiff_file) as src:\n",
    "        # Recortar o raster com a geometria do polígono\n",
    "        bbox = [{\n",
    "            'type': 'Polygon',\n",
    "            'coordinates': [[\n",
    "                (minx, miny),\n",
    "                (minx, maxy),\n",
    "                (maxx, maxy),\n",
    "                (maxx, miny),\n",
    "                (minx, miny)\n",
    "            ]]\n",
    "        }]\n",
    "        try:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, bbox, crop=True, nodata=np.nan)\n",
    "            # Calcular a média dos valores dentro do polígono\n",
    "            mean_value = np.nanmean(out_image)\n",
    "        except:\n",
    "            return 0\n",
    "    return mean_value\n",
    "\n",
    "def create_grid_with_contours(gdf, n_cols, n_rows, tiff_file):\n",
    "    # Obter limites do shapefile\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    #xmin, ymin, xmax, ymax = [-53.11011153, -41, -28.84763991, -14.23318067]\n",
    "    # Calcular o tamanho dos retângulos\n",
    "    dx = (xmax - xmin) / n_cols\n",
    "    dy = (ymax - ymin) / n_rows\n",
    "\n",
    "    # Lista para armazenar polígonos\n",
    "    polygons = []\n",
    "    mean_val = []\n",
    "    x_min = []\n",
    "    x_max = []\n",
    "    y_min = []\n",
    "    y_max = []\n",
    "    # Loop para criar os polígonos\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            x0 = xmin + j * dx\n",
    "            y0 = ymin + i * dy\n",
    "            x1 = x0 + dx\n",
    "            y1 = y0 + dy\n",
    "            \n",
    "            x_min.append(x0)\n",
    "            y_min.append(y0)\n",
    "            x_max.append(x1)\n",
    "            y_max.append(y1)\n",
    "            polygons.append(Polygon([(x0, y0), (x1, y0), (x1, y1), (x0, y1)]))\n",
    "            mean_val.append(calculate_tiff_mean(tiff_file,x0, y0, x1, y1))\n",
    "    # Criar GeoDataFrame a partir dos polígonos\n",
    "    grid_gdf = gpd.GeoDataFrame(geometry=polygons, crs=gdf.crs)\n",
    "    grid_gdf['x_min'] = x_min\n",
    "    grid_gdf['y_min'] = y_min\n",
    "    grid_gdf['x_max'] = x_max\n",
    "    grid_gdf['y_max'] = y_max\n",
    "    grid_gdf['mean_val'] = mean_val\n",
    "    # Interseção com o shapefile original\n",
    "    grid_gdf = gpd.overlay(grid_gdf, gdf, how='intersection')\n",
    "\n",
    "    return grid_gdf\n",
    "\n",
    "# Caminho para o arquivo .shp\n",
    "tiff_file = 'OUTPUT\\MAXENT-IMAGES\\probability_1.tif'\n",
    "\n",
    "# Número de colunas e linhas na malha\n",
    "n_cols = 50\n",
    "n_rows = 50\n",
    "\n",
    "# Criar a malha quadriculada com contornos\n",
    "grid_with_contours = create_grid_with_contours(geoframe.cx[:-42,:], n_cols, n_rows, tiff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar a malha quadriculada com contornos\n",
    "grid_with_contours.plot(column='mean_val', edgecolor='black', cmap='viridis', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotar o heatmap com Plotly Express\n",
    "fig = px.choropleth_mapbox(grid_with_contours, geojson=grid_with_contours.geometry.__geo_interface__,\n",
    "                           locations=grid_with_contours.index, color='mean_val',\n",
    "                           color_continuous_scale=\"viridis\",\n",
    "                           range_color=(0, 1),\n",
    "                           center={\"lat\": -16.95, \"lon\": -47.78},\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=3,\n",
    "                           opacity=1,\n",
    "                           labels={'mean_val': 'Mean Value'}\n",
    "                          )\n",
    "# Adicionar contorno dos polígonos\n",
    "OCCURANCE_FILE = \"assets/INPUT/occurence.parquet\"\n",
    "occurence_df = pd.read_parquet(OCCURANCE_FILE)\n",
    "with open(\"assets/INPUT/occurences.json\") as jfile:\n",
    "    geo_json = json.load(jfile)\n",
    "fig.update_traces(marker_line_width=0)\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "        lat=occurence_df['Latitude'],\n",
    "        lon=occurence_df['Longitude'],\n",
    "        mode='markers',\n",
    "        marker=go.scattermapbox.Marker(\n",
    "            size=8,\n",
    "            color='rgb(0, 0, 0)',\n",
    "            opacity=1\n",
    "        ),\n",
    "        hoverinfo='none'\n",
    "    ))\n",
    "\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "        lat=occurence_df['Latitude'],\n",
    "        lon=occurence_df['Longitude'],\n",
    "        mode='markers',\n",
    "        marker=go.scattermapbox.Marker(\n",
    "            size=6,\n",
    "            color='rgb(194, 33, 36)',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        hoverinfo='none'\n",
    "))\n",
    "\n",
    "fig.update_geos(\n",
    "    visible=False,\n",
    "    resolution=50,\n",
    "    lataxis_gridcolor=\"red\",\n",
    "    lataxis_showgrid=True,\n",
    "    lataxis_dtick=15,\n",
    "    lonaxis_showgrid=True,\n",
    "    lonaxis_dtick=15,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotit(distr_averaged, \"Species Suitability\", cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_AND_TREATED_DATA = \"assets/INPUT/features.parquet\"\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "df = pd.read_parquet(FEATURES_AND_TREATED_DATA)\n",
    "df = df.dropna()\n",
    "\n",
    "y = df.pop(\"Presence\")\n",
    "X = df.copy()\n",
    "\n",
    "def apply_pca(X, standardize=True):\n",
    "    # Standardize\n",
    "    if standardize:\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Create principal components\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    # Create loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,  # transpose the matrix of loadings\n",
    "        columns=component_names,  # so the columns are the principal components\n",
    "        index=X.columns,  # and the rows are the original features\n",
    "    )\n",
    "    return pca, X_pca, loadings\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "pca, X_pca, loadings = apply_pca(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variance(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores=make_mi_scores(X, y)\n",
    "fig = go.Figure([go.Bar(\n",
    "    x=mi_scores.index, y=mi_scores.values\n",
    ")])\n",
    "fig.update_layout(\n",
    "    \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validando multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de multicolinearidade\n",
    "\n",
    "Analisando se os parâmetros são os mesmos nas condições de fitar o modelo com ou sem uma das variáveis.\n",
    "\n",
    "Se eles são não correlacionados, então os coeficientes devem ser iguais para o caso em que ambos estão x quando X1 está e quando ambos estão x quando X2 está\n",
    "\n",
    "$$\n",
    "SSR(X_1|X_2) = SSR(X_1) = SSE(X_2) - SSE(X_1,X_2)\n",
    "$$\n",
    "SSR(X)\n",
    "\n",
    "##\n",
    "Quando existem duas variáveis independentes correlacionadas, os coeficientes da regressão dependem se um ou outra variável está inclusida no modelo\n",
    "\n",
    "- $SSR(X_1)$: Redução de variação da variável resposta ao introduzir a variável $X_1$.\n",
    "- $SSR(X_1|X_2)$: Corresponde a redução de variação da resposta ao introduzir a variável $X_1$ dado que a $X_2$ já está no modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web-scraping-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
